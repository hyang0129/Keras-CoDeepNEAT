{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAM_CODEEPNEAT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kjb1xGvtGUbV5n2gBBIyUPaQ7ViYV96M",
      "authorship_tag": "ABX9TyNGU8l5LUELgEqHqFrlgSv8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyang0129/Keras-CoDeepNEAT/blob/master/LAM_CODEEPNEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH2WpwB1C6z9"
      },
      "source": [
        "c = '''git init .\n",
        "git remote add -t \\* -f origin https://github.com/hyang0129/Keras-CoDeepNEAT\n",
        "git checkout master\n",
        "git reset --hard origin/master\n",
        "'''\n",
        "\n",
        "import os \n",
        "os.system(c)\n",
        "\n",
        "c = \"\"\"git pull\n",
        "git reset --hard origin/master\"\"\"\n",
        "os.system(c)\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,os.getcwd())\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajftGRDqIyFE",
        "outputId": "37144227-b4e4-4189-aede-0c12e03d79ef"
      },
      "source": [
        "!sudo apt-get install graphviz graphviz-dev"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'libgraphviz-dev' instead of 'graphviz-dev'\n",
            "graphviz is already the newest version (2.40.1-2).\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libgvc6-plugins-gtk libxdot4\n",
            "Suggested packages:\n",
            "  gvfs\n",
            "The following NEW packages will be installed:\n",
            "  libgail-common libgail18 libgraphviz-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgvc6-plugins-gtk libxdot4\n",
            "0 upgraded, 8 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 2,120 kB of archives.\n",
            "After this operation, 7,128 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libxdot4 amd64 2.40.1-2 [15.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgvc6-plugins-gtk amd64 2.40.1-2 [18.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgraphviz-dev amd64 2.40.1-2 [57.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n",
            "Fetched 2,120 kB in 2s (1,365 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 160980 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../2-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../3-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libxdot4.\n",
            "Preparing to unpack .../4-libxdot4_2.40.1-2_amd64.deb ...\n",
            "Unpacking libxdot4 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvc6-plugins-gtk.\n",
            "Preparing to unpack .../5-libgvc6-plugins-gtk_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvc6-plugins-gtk (2.40.1-2) ...\n",
            "Selecting previously unselected package libgraphviz-dev.\n",
            "Preparing to unpack .../6-libgraphviz-dev_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgraphviz-dev (2.40.1-2) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../7-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Setting up libxdot4 (2.40.1-2) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgvc6-plugins-gtk (2.40.1-2) ...\n",
            "Setting up libgraphviz-dev (2.40.1-2) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxwTfiFfI-IG",
        "outputId": "ecdb4db7-5210-4e4b-8c24-239de15d4669"
      },
      "source": [
        "!pip install pygraphviz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygraphviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/d6/2c56f09ee83dbebb62c40487e4c972135661b9984fec9b30b77fb497090c/pygraphviz-1.7.zip (118kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 20kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 40kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 8.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pygraphviz\n",
            "  Building wheel for pygraphviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygraphviz: filename=pygraphviz-1.7-cp37-cp37m-linux_x86_64.whl size=166115 sha256=b33ee9bbd4df89c69e139430a831ede0f69012c5a4056de02cfe2f689e33946e\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/59/00/14934a4292c4359eeabcdbf90f33d309b55d0f1be8a1262523\n",
            "Successfully built pygraphviz\n",
            "Installing collected packages: pygraphviz\n",
            "Successfully installed pygraphviz-1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95mlNP3sHJh1",
        "outputId": "d163f127-a4d3-42dd-867c-887973ab046a"
      },
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.5\n",
            "  Using cached https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl\n",
            "Collecting tensorflow==1.13.1\n",
            "  Using cached https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting Networkx==2.3\n",
            "  Using cached https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip\n",
            "Collecting PyDot==1.4.1\n",
            "  Using cached https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
            "Collecting GraphViz==0.11.1\n",
            "  Using cached https://files.pythonhosted.org/packages/5c/b1/016e657586843f40b4daa66127ce1ee9e3285ff15baf5d80946644a98aeb/graphviz-0.11.1-py2.py3-none-any.whl\n",
            "Collecting scikit-learn==0.21.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->-r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 2)) (3.12.4)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 37.9MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 2)) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from Networkx==2.3->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from PyDot==1.4.1->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1->-r requirements.txt (line 2)) (54.2.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Building wheels for collected packages: Networkx\n",
            "  Building wheel for Networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556408 sha256=1a82d4178be4006f39f81330b159529e821fe8764bfaae10484430278f9038ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n",
            "Successfully built Networkx\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, keras, mock, tensorflow-estimator, tensorboard, tensorflow, Networkx, PyDot, GraphViz, scikit-learn\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: networkx 2.5\n",
            "    Uninstalling networkx-2.5:\n",
            "      Successfully uninstalled networkx-2.5\n",
            "  Found existing installation: pydot 1.3.0\n",
            "    Uninstalling pydot-1.3.0:\n",
            "      Successfully uninstalled pydot-1.3.0\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed GraphViz-0.11.1 Networkx-2.3 PyDot-1.4.1 keras-2.2.5 keras-applications-1.0.8 mock-4.0.3 scikit-learn-0.21.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXEwPqv1HnZD"
      },
      "source": [
        "# import sys\n",
        "# sys.path.append(\"..\")\n",
        "# sys.path.insert(0,os.getcwd())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v57863NsIEgs",
        "outputId": "941b537c-0b97-4895-d9a5-7d383120ab6e"
      },
      "source": [
        "import keras, logging, random, pydot, copy, uuid, os, csv, json\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from enum import Enum, auto\n",
        "from typing import List\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.preprocessing import scale\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
        "from keras import regularizers\n",
        "import imp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfNrq44eIS_k"
      },
      "source": [
        "kerascodeepneat = imp.load_source(\"kerascodeepneat\", \"./base/kerascodeepneat.py\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn9K9OzMIYLd"
      },
      "source": [
        "def run_cifar10_full(generations, training_epochs, population_size, blueprint_population_size, module_population_size, n_blueprint_species, n_module_species, final_model_training_epochs):\n",
        "    from keras.datasets import cifar10\n",
        "\n",
        "    #Set parameter tables\n",
        "    global_configs = {\n",
        "        \"module_range\" : ([1, 3], 'int'),\n",
        "        \"component_range\" : ([1, 3], 'int')\n",
        "    }\n",
        "    input_configs = {\n",
        "    \"module_range\" : ([1, 1], 'int'),\n",
        "    \"component_range\" : ([1, 1], 'int')\n",
        "    }\n",
        "    output_configs = {\n",
        "        \"module_range\" : ([1, 1], 'int'),\n",
        "        \"component_range\" : ([1, 1], 'int')\n",
        "    }\n",
        "\n",
        "    possible_components = {\n",
        "        \"conv2d\": (keras.layers.Conv2D, {\"filters\": ([16,48], 'int'), \"kernel_size\": ([1, 3, 5], 'list'), \"strides\": ([1], 'list'), \"data_format\": (['channels_last'], 'list'), \"padding\": (['same'], 'list'), \"activation\": ([\"relu\"], 'list')}),\n",
        "        #\"dense\": (keras.layers.Dense, {\"units\": ([8, 48], 'int')})\n",
        "    }\n",
        "    possible_inputs = {\n",
        "        \"conv2d\": (keras.layers.Conv2D, {\"filters\": ([16,64], 'int'), \"kernel_size\": ([1], 'list'), \"activation\": ([\"relu\"], 'list')})\n",
        "    }\n",
        "    possible_outputs = {\n",
        "        \"dense\": (keras.layers.Dense, {\"units\": ([32,256], 'int'), \"activation\": ([\"relu\"], 'list')})\n",
        "    }\n",
        "\n",
        "    possible_complementary_components = {\n",
        "        #\"maxpooling2d\": (keras.layers.MaxPooling2D, {\"pool_size\": ([2], 'list')}),\n",
        "        \"dropout\": (keras.layers.Dropout, {\"rate\": ([0, 0.5], 'float')})\n",
        "    }\n",
        "    possible_complementary_inputs = None\n",
        "    possible_complementary_outputs = {\n",
        "        \"dense\": (keras.layers.Dense, {\"units\": ([10,10], 'int'), \"activation\": ([\"softmax\"], 'list')})\n",
        "    }\n",
        "\n",
        "    \n",
        "    # The data, split between train and test sets:\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    print('x_train shape:', x_train.shape)\n",
        "    print(x_train.shape[0], 'train samples')\n",
        "    print(x_test.shape[0], 'test samples')\n",
        "\n",
        "    num_classes = 10\n",
        "\n",
        "    # Convert class vectors to binary class matrices.\n",
        "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    validation_split = 0.15\n",
        "    #training\n",
        "    batch_size = 128\n",
        "\n",
        "    #data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        )\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    my_dataset = kerascodeepneat.Datasets(training=[x_train, y_train], test=[x_test, y_test])\n",
        "    my_dataset.SAMPLE_SIZE = 20000\n",
        "    my_dataset.TEST_SAMPLE_SIZE = 2000\n",
        "\n",
        "    logging.basicConfig(filename='execution.log',\n",
        "                        filemode='w+', level=logging.INFO,\n",
        "                        format='%(levelname)s - %(asctime)s: %(message)s')\n",
        "    logging.addLevelName(21, \"TOPOLOGY\")\n",
        "\n",
        "    logging.warning('This will get logged to a file')\n",
        "    logging.info(f\"Hi, this is a test run.\")\n",
        "\n",
        "    compiler = {\"loss\":\"categorical_crossentropy\", \"optimizer\":\"keras.optimizers.Adam(lr=0.005)\", \"metrics\":[\"accuracy\"]}\n",
        "\n",
        "    # Set configurations for full training session (final training)\n",
        "    es = EarlyStopping(monitor='val_acc', mode='auto', verbose=1, patience=15)\n",
        "    mc = ModelCheckpoint('best_model_checkpoint.h5', monitor='val_accuracy', mode='auto', verbose=1, save_best_only=True)\n",
        "    csv_logger = CSVLogger('training.csv')\n",
        "    custom_fit_args = {\"generator\": datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "    \"steps_per_epoch\": x_train.shape[0] // batch_size,\n",
        "    \"epochs\": training_epochs,\n",
        "    \"verbose\": 1,\n",
        "    \"validation_data\": (x_test,y_test),\n",
        "    \"callbacks\": [es, csv_logger]\n",
        "    }                        \n",
        "    improved_dataset = kerascodeepneat.Datasets(training=[x_train, y_train], test=[x_test, y_test])\n",
        "    improved_dataset.custom_fit_args = custom_fit_args\n",
        "    my_dataset.custom_fit_args = None\n",
        "\n",
        "    # Initiate population\n",
        "    population = kerascodeepneat.Population(my_dataset, input_shape=x_train.shape[1:], population_size=population_size, compiler=compiler)\n",
        "  \n",
        "    # Start with random modules\n",
        "    population.create_module_population(module_population_size, global_configs, possible_components, possible_complementary_components)\n",
        "    population.create_module_species(n_module_species)\n",
        "\n",
        "    # Start with random modules\n",
        "    population.create_blueprint_population(blueprint_population_size,\n",
        "                                            global_configs, possible_components, possible_complementary_components,\n",
        "                                            input_configs, possible_inputs, possible_complementary_inputs,\n",
        "                                            output_configs, possible_outputs, possible_complementary_outputs)\n",
        "    population.create_blueprint_species(n_blueprint_species)\n",
        "\n",
        "    # Iterate generating, fitting, scoring, speciating, reproducing and mutating.\n",
        "    iteration = population.iterate_generations(generations=generations,\n",
        "                                                training_epochs=training_epochs,\n",
        "                                                validation_split=validation_split,\n",
        "                                                mutation_rate=0.5,\n",
        "                                                crossover_rate=0.2,\n",
        "                                                elitism_rate=0.1,\n",
        "                                                possible_components=possible_components,\n",
        "                                                possible_complementary_components=possible_complementary_components)\n",
        "\n",
        "    print(\"Best fitting: (Individual name, Blueprint mark, Scores[test loss, test acc], History).\\n\", (iteration))\n",
        "\n",
        "    # Return the best model\n",
        "    best_model = population.return_best_individual()\n",
        "\n",
        "    # Set data augmentation for full training\n",
        "    population.datasets = improved_dataset\n",
        "    print(\"Using data augmentation.\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Best fitting model chosen for retraining: {best_model.name}\")\n",
        "        population.train_full_model(best_model, final_model_training_epochs, validation_split, custom_fit_args)\n",
        "    except:\n",
        "        population.individuals.remove(best_model)\n",
        "        best_model = population.return_best_individual()\n",
        "        print(f\"Best fitting model chosen for retraining: {best_model.name}\")\n",
        "        population.train_full_model(best_model, final_model_training_epochs, validation_split, custom_fit_args)\n",
        "  \n",
        "  \n",
        "def create_dir(dir):\n",
        "    if not os.path.exists(os.path.dirname(dir)):\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(dir))\n",
        "        except OSError as exc: # Guard against race condition\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsBC29jLIcMU",
        "outputId": "94292eb2-65bc-4490-ef2e-d82fa0260b90"
      },
      "source": [
        "generations = 2\n",
        "training_epochs = 2\n",
        "final_model_training_epochs = 2\n",
        "population_size = 1\n",
        "blueprint_population_size = 10\n",
        "module_population_size = 30\n",
        "n_blueprint_species = 3\n",
        "n_module_species = 3\n",
        "\n",
        "\n",
        "create_dir(\"models/\")\n",
        "create_dir(\"images/\")\n",
        "\n",
        "run_cifar10_full(generations, training_epochs, population_size, blueprint_population_size, module_population_size, n_blueprint_species, n_module_species, final_model_training_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Generating 1 components\n",
            "Generating 3 components\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
            "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
            "  if not cb.iterable(width):\n",
            "/usr/local/lib/python3.7/dist-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: \n",
            "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
            "  if cb.iterable(node_size):  # many node sizes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating 3 components\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 3 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 3 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 1 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 3 components\n",
            "Generating 3 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 3 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 1 components\n",
            "Generating 2 components\n",
            "Generating 2 components\n",
            "Generating 3 components\n",
            "Generating 1 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
            "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
            "  if not cb.iterable(width):\n",
            "/usr/local/lib/python3.7/dist-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: \n",
            "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
            "  if cb.iterable(node_size):  # many node sizes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating 3 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 3 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 1 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 3 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 1 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 3 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 3 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 2 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            "Generating 1 modules\n",
            "Generating 1 components\n",
            "Generating 1 components\n",
            " -- Iterating generation 0 -- \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
            "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
            "  if not cb.iterable(width):\n",
            "/usr/local/lib/python3.7/dist-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: \n",
            "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
            "  if cb.iterable(node_size):  # many node sizes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 14839 samples, validate on 2619 samples\n",
            "Epoch 1/2\n",
            "11136/14839 [=====================>........] - ETA: 1:15 - loss: 2.3374 - acc: 0.1000"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}